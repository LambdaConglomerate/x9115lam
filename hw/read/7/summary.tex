\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{babel}
\begin{document}
\title{Paper Seven Summary}


\author{Matthew Neal, Joseph Sankar, and Alexander Sobran}

\maketitle

\section*{Reference}

Rahman et al. \cite{Rahman} listed below.


\section*{Important Keywords}
\begin{description}
\item [{Cross-project defect prediction}] Using data from one project to predict defects in another.
\item [{Within-project defect prediction}] Using data from previous releases of a project to predict defects in future releases. For new projects, the lack of historical defect data makes this kind of defect prediction almost impossible.
\item[{F-measure}] The harmonic mean of precision and recall. A unified score used to balance the trade-off between precision and recall.
\item[{Over-fitting}] When a model has completely learned all the variances of the training data and has lost significant generality leading to significantly worse predictive performance on unseen test data. This property usually manifests as very low training error and very high test error. Over-fitting becomes more probable as a model gains greater complexity.

\end{description}

\section*{Feature Extraction}
\begin{description}
\item[{Motivational Statements}]  While within-project defect prediction can be very effective, new projects don't have the volume of data needed to create these models. Cross-project defect prediction models aim to help with this issue, but so far the results have largely been disappointing. The authors hope to show that cross-project defect prediction can be roughly as effective as traditional defect prediction by using a different set of measures, namely those based on a variety of tradeoffs of time-and-cost vs. quality.
\item[{Hypothesis}] The hypothesis of this paper is that performance of cross-project defect prediction models is similar to the performance of within-project defect prediction when assessing performance within reasonable and practical resource limitations.
\item[{Statistical tests}] The authors used the area under the ROC curve (AUC), AUCCE, precision, recall, and f-measure statistical tests to report their results. They varied the cutoffs for precision, recall, and f-measure but used the standard 0.5 cutoff for their sanity checks.
\item[{Informative Visualization}] Figure 2 gives an informative comparison of cross and within-project model performance assessed by multiple performance metrics. It visualizes how well the distribution of results overlap for within and cross-project model performance even though within-project generally has higher performance. The addition of p-values shows how the differences lack significance. Figure 4 is also very informative, showing how weighting the defects by density affects AUC. Both cross-project and within-project have similar results and the addition of random and optimal baselines are useful for comparison.

\end{description}


\section*{Possible Improvements}
\begin{itemize}
\item In the Results section, some of the graphs are hard to read because they have been scaled down to fit on the page. Look at Figures 5 and 7 for example. The y-axis label runs into the labels along the y-axis. To fix this, the individual graphs should be split so that there is only one per column, or they should be combined and made bigger.
\item The authors could have reported the importance of their features in predicting defects using a measurement such as entropy. This could aid other researchers when selecting features.
\item The data mining methods used to extract features are summarized generally. The process can actual be very complex and depend on many assumptions. A specific and more detailed explanation could be more useful for future research.
\end{itemize}

\section*{Connection to Other Papers}
The authors state that they are examining code at the file level. They reference our first paper on ecological inference \cite{Posnett} when stating that they feared that a model on a coarser level of aggregation may not be a good predictor at the file level. 

\bibliographystyle{plain}
\bibliography{references}
\end{document}